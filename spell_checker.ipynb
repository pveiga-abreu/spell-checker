{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "difficult-affiliate",
   "metadata": {},
   "source": [
    "### Tokenize articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "disturbed-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numerical-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant\n",
    "ALPHABET = 'abcdefghijklmopqrstuvwxyzáâàãéêèíîìóôòõúûùç'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "royal-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('artigos.txt', 'r', encoding='utf8') as fil:\n",
    "    articles = fil.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sensitive-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "careful-conditions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515907\n",
      "['imagem', 'Temos', 'a', 'seguinte', 'classe', 'que', 'representa', 'um', 'usuário', 'no', 'nosso', 'sistema', ':', 'java', 'Para', 'salvar', 'um', 'novo', 'usuário', ',', 'várias', 'validações', 'são', 'feitas', ',', 'como', 'por', 'exemplo', ':', 'Ver', 'se', 'o', 'nome', 'só', 'contém', 'letras', ',', '[', '*', '*']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(tokens[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-musician",
   "metadata": {},
   "source": [
    "### Separating words from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complimentary-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_words(tokens: list) -> list:\n",
    "    '''\n",
    "    Generates tokens\n",
    "    '''\n",
    "    return [ token for token in tokens if token.isalpha() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liked-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = separate_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "veterinary-valentine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403106\n",
      "['imagem', 'Temos', 'a', 'seguinte', 'classe', 'que', 'representa', 'um', 'usuário', 'no', 'nosso', 'sistema', 'java', 'Para', 'salvar', 'um', 'novo', 'usuário', 'várias', 'validações', 'são', 'feitas', 'como', 'por', 'exemplo', 'Ver', 'se', 'o', 'nome', 'só', 'contém', 'letras', 'o', 'CPF', 'só', 'números', 'e', 'ver', 'se', 'o']\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(words[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-fleece",
   "metadata": {},
   "source": [
    "### putting all on lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "educated-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(words: list) -> list:\n",
    "    '''\n",
    "    Puts all words on the list in lower case\n",
    "    '''\n",
    "    return [ word.lower() for word in words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "proud-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_words = normalize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extraordinary-museum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403106\n",
      "['imagem', 'temos', 'a', 'seguinte', 'classe', 'que', 'representa', 'um', 'usuário', 'no', 'nosso', 'sistema', 'java', 'para', 'salvar', 'um', 'novo', 'usuário', 'várias', 'validações', 'são', 'feitas', 'como', 'por', 'exemplo', 'ver', 'se', 'o', 'nome', 'só', 'contém', 'letras', 'o', 'cpf', 'só', 'números', 'e', 'ver', 'se', 'o']\n"
     ]
    }
   ],
   "source": [
    "print(len(norm_words))\n",
    "print(norm_words[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-edgar",
   "metadata": {},
   "source": [
    "### total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "smaller-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18465\n"
     ]
    }
   ],
   "source": [
    "print(len(set(norm_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-inclusion",
   "metadata": {},
   "source": [
    "### Slicing and generate new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "front-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letters(slices: list) -> tuple:\n",
    "    '''\n",
    "    Insert all letters of the Portuguese alphabet in the sliced word\n",
    "    '''\n",
    "    generated_words = list()\n",
    "    \n",
    "    for left, right in slices:\n",
    "        for letter in ALPHABET: \n",
    "            generated_words.append(left + letter + right)\n",
    "    \n",
    "    return tuple(generated_words)\n",
    "\n",
    "def generate_words(word: str) -> str:\n",
    "    '''\n",
    "    Generates possible correct keywords \n",
    "    '''\n",
    "    slices = [ (word[:i], word[i:]) for i in range(len(word)+1) ]\n",
    "    \n",
    "    return insert_letters(slices) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-deposit",
   "metadata": {},
   "source": [
    "### Creating the checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "every-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(norm_words)\n",
    "total_words = len(norm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "professional-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(word: str) -> float:\n",
    "    '''\n",
    "    Calculates the probability of the word being correct\n",
    "    '''\n",
    "    return freq[word]/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "looking-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(word: str) -> str:\n",
    "    '''\n",
    "    Checks the probabilities of all words\n",
    "    '''\n",
    "    generated_words = generate_words(word)\n",
    "    correct = max(generated_words, key=probability)\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reflected-filename",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lógica'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check('lgica')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
